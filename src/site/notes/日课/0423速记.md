---
{"dg-publish":true,"permalink":"//0423/","title":"我的第2篇公开笔记"}
---

悲剧，昨天晚上模型下载下来了，但是忘记上传到autodl上了，于是autodl上有需要下载，估计要一上午了，诶，模型太大了，上传和下载都很耽误时间，导致流程被阻塞了。研究了一下，似乎没有更快的办法了，现在几mb/s已经很快了。

趁着这个功夫，可以把其他的model上传一下，又上传了个7b的，别说，还挺快。我要不要把13b的先暂停一下，先把7b的下载了？这样可能快一点。anyway，先添加到下载队列中吧，这种下载任务可真是耽误时间啊。昨天晚上咋就没想起来呢？算了，就这样吧。

去接点水喝，现在正好只有一个人。接水的时候，脑子里突然灵光一闪，也许可以先走后面的环节。研究下如何把大模型包装成openai调用格式。

看了下官方文档，发现部署模型比我想象的更加容易，完全不需要编写啥后端服务。直接用lmdeploy就能启动一个服务。看到的那一堆和openai相关的其实是客户端访问代码。

不过我的模型都是本地的，看起来lmdeploy启动的似乎是远程model，所以需要了解一下这个工具。这个工具支持了推理，部署，量化，但是看起来支持的模型有限，我下载了最新的模型，还不知道是否支持咧。不过推测的话，感觉是应该支持的。可以看下这个工具版本。查看了下，没看到提交记录中关于新model的支持说明。

不过询问gpt说是支持的，因为hugggingface中的示例代码是调用到了，所以推测应该是支持的。

现在应该万事俱备了，只差东风，等待这个设备有gpu了，就可以运行了吧。不过还是可以再做一点事情，就是安装一下环境。行吧，那就去安装一下环境吧。安装完毕了，又重新租了一个服务器，并且在上面下载了一个8b的模型。等待模型下载完毕，然后在这上面试一试吧。

下载完毕，尝试部署服务，解决oom了，内存爆了。

尝试换个设备，但是没有，只好再租了一个服务器，然后把数据拷贝过去，这次是运行llava模型，基本使用pipe推理没问题。但是使用lmdeploy部署报错。搜索了一番发现有人也报了同样的错误，暂时没有合适的解决办法。目前考虑是使用docker方式部署再看看.

突然发现不行，因为autodl本身就是一个docker。docker in docker比较费劲。

还有什么别的办法吗？那就是不使用这个lmdeploy部署了呗。采用了llava的官网部署，但是也还是没能成功，报错了。哎。




